{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20200723_Plant_Analyzer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbGYdt_cfZID",
        "colab_type": "text"
      },
      "source": [
        "__Disclaimer__: Welcome to the Emotion Tracker developed in Collaboration with \n",
        "Prof. Peter Gloor and the support from Josephine Van Delden. This is an alpha version of the upcoming web service."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrDOX74EnAWj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Step 0: Upload Video\n",
        "\n",
        "# Imports\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "import librosa\n",
        "import librosa.display\n",
        "import altair as alt\n",
        "from scipy.signal import chirp, find_peaks, peak_widths\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "!curl -o happy.png https://i.imgur.com/PXpWO5C.png -s\n",
        "!curl -o sad.png https://i.imgur.com/mAYh4Qt.png -s\n",
        "# !curl -L https://www.dropbox.com/s/yk0es4189d9bc6s/plants_alternating_emotions_w_60s_silence%2Bday.mov?dl=1 -o alternating_60s_new.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/yncd1m5b462qxbk/plants_alternating_emotions_w_10s_silence.mov?dl=1 -o alternating_10s.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/dvj63tdba0ib2xk/plants_alternating_emotions_w_60s_silence.mov?dl=1 -o alternating_60s.mov -s\n",
        "!curl -L https://www.dropbox.com/s/yrlxynmnr9aqvsf/paprika_lq.mov?dl=1 -o paprika.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/j5a18aw661qz8is/pot_plastic.mov?dl=1 -o pot.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/1o60nbvionp4uu0/mimosa_nosound-400x.mp4?dl=1 -o mimosa.mov -s\n",
        "!curl -L https://www.dropbox.com/s/dun532o3s1c49xi/Codariocalyx%2BMimosa-400x.mp4?dl=1 -o mimosa_400x.mov -s\n",
        "!curl -L https://www.dropbox.com/s/lb24eyw57qdiocc/codariocalyx_nopeter_nosound-400x.mp4?dl=1 -o codariocalyx_no_peter.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/1o60nbvionp4uu0/mimosa_nosound-400x.mp4?dl=1 -o mimosa_400x_no_sound.mov -s\n",
        "\n",
        "print(\"Testvideos preloaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOtPzivUfYFl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Step 1: Choose Hyperparameters\n",
        "\n",
        "# Clean up old files save new ones.\n",
        "try:\n",
        "    os.remove(\"pls_delete.mp4\")\n",
        "    os.remove(\"pls_delete.mpeg\")\n",
        "    os.remove(\"pls_delete.wav\")\n",
        "    os.remove(\"pls_delete_play.mp4\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Files already deleted.\")\n",
        "else:\n",
        "    print(\"Files removed.\")\n",
        "\n",
        "\n",
        "# Constants\n",
        "\n",
        "INPUT_VIDEO_NAME = 'pls_delete'\n",
        "OUTPUT_VIDEO_NAME_MPEG = 'pls_delete.mpeg'\n",
        "OUTPUT_VIDEO_NAME_MP4 = '{}.mp4'.format(INPUT_VIDEO_NAME)\n",
        "OUTPUT_VIDEO_NAME_WAV = '{}.wav'.format(INPUT_VIDEO_NAME)\n",
        "FRAMES_PER_SECOND = 30\n",
        "\n",
        "# Params\n",
        "\n",
        "Video = \"paprika.mov\"  #@param ['paprika.mov', 'pot.mov', 'mimosa.mov', 'mimosa_400x.mov', 'codariocalyx_no_peter.mov', 'mimosa_400x_no_sound.mov']\n",
        "INPUT_VIDEO = Video\n",
        "Interesting_Points =   2#@param {type:\"integer\"}\n",
        "NO_OF_ROIS = (Interesting_Points-1)\n",
        "Sound_Features =  4#@param {type:\"integer\"}\n",
        "NO_OF_MFCC = Sound_Features\n",
        "Shrink_Regions_by_px =  6#@param {type:\"integer\"}\n",
        "ERODE_1 =  Shrink_Regions_by_px\n",
        "ERODE_2 = ERODE_1 \n",
        "\n",
        "# Dilate: Makes intersting regions grow.\n",
        "Grow_Regions_by_px =  18#@param {type:\"integer\"}\n",
        "DILATE_1 = Grow_Regions_by_px\n",
        "DILATE_2 = DILATE_1\n",
        "\n",
        "Lumen_Threshold = 200#@param {type:\"integer\"} \n",
        "MASK_THRESH = Lumen_Threshold\n",
        "Countour_Approximation = 60#@param {type:\"integer\"}\n",
        "CONTOUR_AREA = Countour_Approximation\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "class Leaf():\n",
        "    def __init__(self, id, hsv_frame, track_window):\n",
        "\n",
        "        self.id = id\n",
        "\n",
        "        self.track_window = track_window\n",
        "        self.term_crit = \\\n",
        "            (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 10, 1)\n",
        "\n",
        "        # Initialize the histogram.\n",
        "        self.x, self.y, self.w, self.h = track_window\n",
        "        roi = hsv_frame[y:y+h, x:x+w]\n",
        "        roi_hist = cv2.calcHist([roi], [0], None, [16], [0, 180])\n",
        "        self.roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255,\n",
        "                                      cv2.NORM_MINMAX)\n",
        "\n",
        "        # Initialize the Kalman filter.\n",
        "        self.kalman = cv2.KalmanFilter(4, 2)\n",
        "        self.kalman.measurementMatrix = np.array(\n",
        "            [[1, 0, 0, 0],\n",
        "             [0, 1, 0, 0]], np.float32)\n",
        "        self.kalman.transitionMatrix = np.array(\n",
        "            [[1, 0, 1, 0],\n",
        "             [0, 1, 0, 1],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 0, 1]], np.float32)\n",
        "        self.kalman.processNoiseCov = np.array(\n",
        "            [[1, 0, 0, 0],\n",
        "             [0, 1, 0, 0],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 0, 1]], np.float32) * 0.03\n",
        "        cx = x+w/2\n",
        "        cy = y+h/2\n",
        "        self.kalman.statePre = np.array(\n",
        "            [[cx], [cy], [0], [0]], np.float32)\n",
        "        self.kalman.statePost = np.array(\n",
        "            [[cx], [cy], [0], [0]], np.float32)\n",
        "        \n",
        "    def get_plant_values(self): \n",
        "        return np.array([ self.id, self.x, self.y, self.w, self.h ])\n",
        "    \n",
        "    def update(self, frame, hsv_frame):\n",
        "\n",
        "        back_proj = cv2.calcBackProject(\n",
        "            [hsv_frame], [0], self.roi_hist, [0, 180], 1)\n",
        "\n",
        "        ret, self.track_window = cv2.meanShift(\n",
        "            back_proj, self.track_window, self.term_crit)\n",
        "        x, y, w, h = self.track_window\n",
        "        center = np.array([x+w/2, y+h/2], np.float32)\n",
        "\n",
        "        prediction = self.kalman.predict()\n",
        "        estimate = self.kalman.correct(center)\n",
        "        center_offset = estimate[:,0][:2] - center\n",
        "        self.track_window = (x + int(center_offset[0]),\n",
        "                             y + int(center_offset[1]), w, h)\n",
        "        x, y, w, h = self.track_window\n",
        "\n",
        "        # Draw the predicted center position as a blue circle.\n",
        "        cv2.circle(frame, (int(prediction[0]), int(prediction[1])),\n",
        "                   4, (255, 0, 0), -1)\n",
        "\n",
        "        # Draw the corrected tracking window as a cyan rectangle.\n",
        "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "        # Draw the ID above the rectangle in blue text.\n",
        "        cv2.putText(frame, 'ID: %d' % self.id, (x, y-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0),\n",
        "                    1, cv2.LINE_AA)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4EkbtzAglB0",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title # Step 2: Feature Engineering\n",
        "\n",
        "print(\"Starting time: {}\".format(datetime.now()))\n",
        "\n",
        "# Create a VideoCapture object\n",
        "import time\n",
        "start_time = time.time()\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
        "arr = []\n",
        "no_of_frame = 0\n",
        "\n",
        "# Default resolutions of the frame are obtained and casted to int.\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "# Create the KNN background subtractor.\n",
        "bg_subtractor = cv2.createBackgroundSubtractorKNN()\n",
        "history_length = 20\n",
        "bg_subtractor.setHistory(history_length)\n",
        "\n",
        "erode_kernel = cv2.getStructuringElement(\n",
        "    cv2.MORPH_ELLIPSE, (ERODE_1, ERODE_2))\n",
        "dilate_kernel = cv2.getStructuringElement(\n",
        "    cv2.MORPH_ELLIPSE, (DILATE_1, DILATE_2))\n",
        "leafs = []\n",
        "num_history_frames_populated = 0\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_NAME_MPEG, fourcc, FRAMES_PER_SECOND, (frame_width,frame_height))\n",
        "\n",
        "grabbed_frames = 0\n",
        "\n",
        "while True:\n",
        "    grabbed, frame = cap.read()\n",
        "    grabbed_frames += 1\n",
        "    # print(grabbed_frames)\n",
        "    if (grabbed is False):\n",
        "        break\n",
        "\n",
        "    no_of_frame = no_of_frame + 1\n",
        "\n",
        "    # Apply the KNN background subtractor.\n",
        "    fg_mask = bg_subtractor.apply(frame)\n",
        "\n",
        "    # Let the background subtractor build up a history.\n",
        "    if num_history_frames_populated < history_length:\n",
        "        num_history_frames_populated += 1\n",
        "        continue\n",
        "\n",
        "    # resources: https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html\n",
        "\n",
        "    # Try this out:\n",
        "    # th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
        "    #         cv.THRESH_BINARY,11,2)\n",
        "\n",
        "    # Create the thresholded image.\n",
        "    # MASK-Threshold\n",
        "    _, thresh = cv2.threshold(fg_mask, MASK_THRESH, 255,\n",
        "                                cv2.THRESH_BINARY)\n",
        "    cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "    cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "\n",
        "    # Detect contours in the thresholded image.\n",
        "    if OPENCV_MAJOR_VERSION >= 4:\n",
        "        # OpenCV 4 or a later version is being used.\n",
        "        contours, hier = cv2.findContours(\n",
        "            thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    else:\n",
        "        _, contours, hier = cv2.findContours(\n",
        "            thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Draw green rectangles around large contours.\n",
        "    # Also, if no leafs are being tracked yet, create some.\n",
        "    should_initialize_leafs = len(leafs) == 0\n",
        "    id = 0\n",
        "    for c in contours:\n",
        "        if cv2.contourArea(c) > CONTOUR_AREA:\n",
        "            (x, y, w, h) = cv2.boundingRect(c)\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h),\n",
        "                            (0, 255, 0), 1)\n",
        "            if should_initialize_leafs:\n",
        "                leafs.append(\n",
        "                    Leaf(id, hsv_frame,\n",
        "                                (x, y, w, h)))\n",
        "        frame_no_and_values = np.array([ id, x, y, w, h ])\n",
        "        a = np.append(frame_no_and_values, [no_of_frame])\n",
        "        arr.append(a)\n",
        "        id += 1\n",
        "        \n",
        "\n",
        "    # Update the tracking of each leaf.\n",
        "    for leaf in leafs:\n",
        "        leaf.update(frame, hsv_frame)\n",
        "        frame_no_and_values = np.array([ id, x, y, w, h ])\n",
        "        a = np.append(frame_no_and_values, [no_of_frame])\n",
        "        arr.append(a)\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "motion_array = np.array(arr)\n",
        "print(\"Processing took\", round(((time.time() - start_time)/60), 2), \"minutes.\")\n",
        "\n",
        "start_time2 = time.time()\n",
        "\n",
        "# To Do: shorten videos to maximal 20s, so that you can watch all of them.\n",
        "\n",
        "subprocess.run('ffmpeg -i {} {}'.format(OUTPUT_VIDEO_NAME_MPEG, OUTPUT_VIDEO_NAME_MP4), shell=True)\n",
        "\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "ffmpeg_extract_subclip('pls_delete.mp4', 0, 10, targetname=\"pls_delete_play.mp4\")\n",
        "\n",
        "print(\"Conversion to mp4 and creating preview took\", round(((time.time() - start_time2)/60), 2), \"minutes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_DT6YZOl6eV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title #Step 3: Preview the Video\n",
        "\n",
        "def playvideo(filename):\n",
        "    video = io.open(filename, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    return HTML(data='''<video width=\"80%\" style=\"display:block; margin:0 auto;\" alt=\"test\" controls>\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "                 </video>'''.format(encoded.decode('ascii')))\n",
        "playvideo('pls_delete_play.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf6Gggtm4zI5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title # Step 4: Data Visualizations\n",
        "\n",
        "# ____ VIDEO PROCESSING ____\n",
        "# This is the output NumPy Array from the video processing\n",
        "data = motion_array\n",
        "\n",
        "# Create a Pandas DataFrame from NumPy arrays with UniqueID, x- & y-Value of ROI (i.e., Region of Interest), as well as width and height of the accompanying moving windows. \n",
        "# Frame refers to video frame (30 fps) on which point was tracked.\n",
        "# RESET DF\n",
        "df_video = pd.DataFrame(data=data[0:,0:], index=[i for i in range(data.shape[0])], columns=['ID','X','Y','W','H','Frame'])\n",
        "\n",
        "# Calculate Elapsed Time for Frames of Video\n",
        "df_video[\"Elapsed\"] = round(df_video[\"Frame\"]/30, 1)\n",
        "\n",
        "# Remove Duplicates\n",
        "df_video_wo_dups = df_video.drop_duplicates(subset=None, keep='first', inplace=False)\n",
        "df_video_queried = df_video_wo_dups.query('0 <= ID <= {}'.format(NO_OF_ROIS))\n",
        "# df_video_4_cols = df_video_queried.filter(items=['ID', 'X', 'Y', 'Elapsed'])\n",
        "df_video_3_cols = df_video_queried.filter(items=['ID', 'Y', 'Elapsed'])\n",
        "\n",
        "# Minding that Matplotlib inverts x/y Values\n",
        "# df_inverted_x_y_minus_1 = df_video_4_cols.sub([0, frame_width, frame_height, 0], axis='columns')\n",
        "df_inverted_x_y_minus_1 = df_video_3_cols.sub([0, frame_height, 0], axis='columns')\n",
        "# df_inverted_x_y = df_inverted_x_y_minus_1.mul([1, -1, -1, 1], axis='columns')\n",
        "df_inverted_x_y = df_inverted_x_y_minus_1.mul([1, -1, 1], axis='columns')\n",
        "df_video = df_inverted_x_y\n",
        "\n",
        "# Use Pivot Feature of Pandas DataFrame\n",
        "# df_pivoted_video = pd.pivot_table(df_video, values=[\"X\", \"Y\"], index=pd.Grouper(key='Elapsed'), columns=[\"ID\"],)\n",
        "df_pivoted_video = pd.pivot_table(df_video, values=[\"Y\"], index=pd.Grouper(key='Elapsed'), columns=[\"ID\"],)\n",
        "\n",
        "# Create flexible number of indices based on number of Regions of Interest\n",
        "int(df_video_queried.max(axis = 0)[0]); counter = 0\n",
        "# x_indices = []\n",
        "y_indices = []\n",
        "while counter <= int(df_video_queried.max(axis = 0)[0]):\n",
        "    # x_indices.append(('X' + str(counter))); \n",
        "    y_indices.append(('Y' + str(counter)))\n",
        "    counter = counter + 1\n",
        "df_pivoted_video.columns = y_indices # + x_indices\n",
        "\n",
        "# ____AUDIO PROCESSING____\n",
        "\n",
        "# Extracts Audio from Input Video\n",
        "command = \"ffmpeg -i {} -ab 160k -ac 2 -ar 44100 -vn {}\".format(INPUT_VIDEO, OUTPUT_VIDEO_NAME_WAV)\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "# Unpack Audio\n",
        "audio_path = INPUT_VIDEO\n",
        "x , sr = librosa.load(audio_path)\n",
        "\n",
        "# Get the MFCCs (and choose the number of features to extract); manipulate Pandas DataFrame.\n",
        "mfccs = librosa.feature.mfcc(x, sr=sr, n_mfcc=NO_OF_MFCC)\n",
        "audio_data = np.transpose(mfccs)\n",
        "\n",
        "# Calculate the step length between a mfcc and the audio in seconds\n",
        "window_length = 512 # default value by mfcc algorithm\n",
        "length = (round((window_length/sr)*mfccs.shape[1]), 2)\n",
        "step_size = length[0]/mfccs.shape[1]\n",
        "\n",
        "# Getting Elapsed Time of Audio from Audiofile\n",
        "audio_time_data = np.arange(0, (length[0]), step_size)\n",
        "audio_time_data = np.round(audio_time_data, 1)\n",
        "df_audio_time_steps = pd.DataFrame(data=audio_time_data, \n",
        "                index=[i for i in range(audio_time_data.shape[0])], \n",
        "                columns=['Elapsed_t'])\n",
        "# Create the Audio Pandas DataFrame\n",
        "df_audio = pd.DataFrame(data=audio_data, \n",
        "                index=[i for i in range(audio_data.shape[0])], \n",
        "                columns=['MFCC'+str(i) for i in range(audio_data.shape[1])])\n",
        "df_audio_w_time = pd.concat([df_audio_time_steps, df_audio], axis=1, join='outer')\n",
        "df_audio_w_time_w_o_dups = df_audio_w_time.drop_duplicates(subset='Elapsed_t', keep='first')\n",
        "df_audio_w_time_w_o_dups\n",
        "\n",
        "## Merge AUDIO and VIDEO\n",
        "# df_merged = pd.concat([df_audio_w_time_w_o_dups, df_pivoted_video], axis = 1, join= 'outer')\n",
        "df_merged = pd.merge(df_audio_w_time_w_o_dups, df_pivoted_video, how='left', left_on=['Elapsed_t'], right_on=['Elapsed']) # add 'indicator=True' to see functionality\n",
        "df_bf_filled = df_merged.fillna(method='bfill') # backward filling (NANs filling up)\n",
        "df_filled = df_bf_filled.fillna(method='ffill') # forward filling (NANs filling down)\n",
        "\n",
        "# transform from wide to long-form data\n",
        "# https://altair-viz.github.io/user_guide/data.html\n",
        "\n",
        "df_filled_every_two_seconds = df_filled.iloc[::20]\n",
        "\n",
        "df_filled_melted = df_filled_every_two_seconds.melt('Elapsed_t', var_name='Variables', value_name='Values')\n",
        "\n",
        "alt.Chart(df_filled_melted).mark_line().encode(\n",
        "  x='Elapsed_t',\n",
        "  y='Values',\n",
        "  color='Variables'\n",
        "  ).properties(\n",
        "    width=900,\n",
        "    height=400,\n",
        ").interactive(bind_y=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJXLWK_WA21",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title # Step 5: Happiness Indicator\n",
        "\n",
        "total_happiness_counter = 0\n",
        "per_curve_happiness_counter = 0\n",
        "img_happy = cv2.imread('happy.png', cv2.IMREAD_UNCHANGED)\n",
        "img_sad = cv2.imread('sad.png', cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "# obviously one cannot assume that the height is fixed\n",
        "HEIGHT = 150\n",
        "# HEIGHT = (max(x[peaks])/2 )\n",
        "\n",
        "# To Do: has to be a regex and start with y(i) values\n",
        "for tracked_move in df_filled.columns[5:6]:\n",
        "    x = df_filled[tracked_move]\n",
        "    # try without height bc its an absolute value\n",
        "    peaks, _ = find_peaks(x, HEIGHT, distance=50)\n",
        "    per_curve_happiness_counter = 0\n",
        "    per_curve_happiness_indicators = len(x[peaks])\n",
        "    happiness_threshold = (max(x[peaks]) + min(x[peaks]))/2\n",
        "    \n",
        "    for value in x[peaks]:\n",
        "        if value > happiness_threshold:\n",
        "            per_curve_happiness_counter = per_curve_happiness_counter + 1\n",
        "            total_happiness_counter = total_happiness_counter + 1\n",
        "\n",
        "if per_curve_happiness_counter > (per_curve_happiness_indicators/2):\n",
        "    print(\"This plant observed that the speaker is >>> happy <<<.\")\n",
        "    cv2_imshow(img_happy)\n",
        "else:\n",
        "    print(\"This plant observed that the speaker is >>> sad <<<.\")\n",
        "    cv2_imshow(img_sad)\n",
        "\n",
        "plt.plot(x)\n",
        "plt.plot(peaks, x[peaks], \"x\")\n",
        "plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7teKtpm0gzxh",
        "colab_type": "text"
      },
      "source": [
        "# Additional Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6CAmpCHTZ3G",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Dummy Sad Plant\n",
        "\n",
        "img_happy = cv2.imread('happy.png', cv2.IMREAD_UNCHANGED)\n",
        "img_sad = cv2.imread('sad.png', cv2.IMREAD_UNCHANGED)\n",
        "print(\"This plant observed that the speaker is >>> sad <<<.\")\n",
        "cv2_imshow(img_sad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZYb3cMZPMLB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Camera Capturing [in Process]\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "\n",
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_q_b5d6g2L7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Export to Excel\n",
        "df_filled.to_excel('11_min_video.xlsx')\n",
        "df_filled_melted.to_excel('11_min_video_melted.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-3S5srS3uVc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Interactive Scatter Plot\n",
        "\n",
        "# df_filled\n",
        "\n",
        "# alt.Chart(df_filled_melted).mark_circle().encode(\n",
        "#     alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
        "#     alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
        "# ).properties(\n",
        "#     width=150,\n",
        "#     height=150\n",
        "# ).repeat(\n",
        "#     row=['Elapsed_t', 'Variables', 'Values'],\n",
        "#     column=['Variables', 'Elapsed_t', 'Values']\n",
        "# ).interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFQ8iyIwkYhy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Check for Movie Duration\n",
        "import moviepy.editor\n",
        "\n",
        "# Converts into more readable format\n",
        "def convert(seconds):\n",
        "    hours = seconds // 3600\n",
        "    seconds %= 3600\n",
        "\n",
        "    mins = seconds // 60\n",
        "    seconds %= 60\n",
        "\n",
        "    return hours, mins, seconds\n",
        "\n",
        "# Duration of Input Video\n",
        "video = moviepy.editor.VideoFileClip(INPUT_VIDEO)\n",
        "video_duration = float(video.duration)\n",
        "hours, mins, secs = convert(video_duration)\n",
        "print(\"The Video named {} Seconds:\".format(INPUT_VIDEO), secs)\n",
        "\n",
        "# Duration of Resulting Video\n",
        "video2 = moviepy.editor.VideoFileClip(OUTPUT_VIDEO_NAME_MP4)\n",
        "video_duration2 = float(video2.duration)\n",
        "hours, mins, secs = convert(video_duration2)\n",
        "print(\"The Video named {} Seconds:\".format(OUTPUT_VIDEO_NAME_MP4), secs)\n",
        "\n",
        "# Cross-check Length of Video and Audio\n",
        "print(\"Length of Audio in Seconds: \", (512/sr)*mfccs.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkQfbg_hTvT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Getting FPS of Video\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "\n",
        "    # Start default camera\n",
        "    video = cv2.VideoCapture(INPUT_VIDEO)\n",
        "\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "\n",
        "    # Release video\n",
        "    video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_IRN0d9iZzt",
        "colab_type": "text"
      },
      "source": [
        "Please send Feedback to sduerr@mit.edu."
      ]
    }
  ]
}