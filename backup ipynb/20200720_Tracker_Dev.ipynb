{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20200720_Tracker_Dev.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbGYdt_cfZID",
        "colab_type": "text"
      },
      "source": [
        "__Disclaimer__: Welcome to the Emotion Tracker developed in Collaboration with \n",
        "Prof. Peter Gloor and the support from Josephine Van Delden. This is an alpha version of the upcoming web service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eILYqGnw6Ba1",
        "colab_type": "text"
      },
      "source": [
        "### Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEYHdmWe5qxc",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# General\n",
        "import time\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "\n",
        "# Data Processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from scipy.signal import chirp, find_peaks, peak_widths\n",
        "import librosa\n",
        "import librosa.display\n",
        "# Thresholding Testing\n",
        "from skimage.data import page\n",
        "from skimage.filters import (threshold_otsu, threshold_niblack, threshold_sauvola)\n",
        "\n",
        "\n",
        "# Visual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import altair as alt\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UPM0MYw5wk2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1d1aa8c-b9e9-4612-fc97-7962dc6ae93c"
      },
      "source": [
        "# Clean Up Temporary Movie Files\n",
        "try:\n",
        "    os.remove(\"pls_delete.mp4\")\n",
        "    os.remove(\"pls_delete.mpeg\")\n",
        "    os.remove(\"pls_delete.wav\")\n",
        "    os.remove(\"pls_delete_play.mp4\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Downloading Videos...\")\n",
        "else:\n",
        "    print(\"RM. Downloading Videos...\")\n",
        "\n",
        "# To Do: Check if files are already downloaded, else download them.\n",
        "\n",
        "# # Downloading\n",
        "# # Videos\n",
        "\n",
        "# !curl -L https://www.dropbox.com/s/yk0es4189d9bc6s/plants_alternating_emotions_w_60s_silence%2Bday.mov?dl=1 -o alternating_60s_new.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/yncd1m5b462qxbk/plants_alternating_emotions_w_10s_silence.mov?dl=1 -o alternating_10s.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/dvj63tdba0ib2xk/plants_alternating_emotions_w_60s_silence.mov?dl=1 -o alternating_60s.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/yrlxynmnr9aqvsf/paprika_lq.mov?dl=1 -o paprika.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/j5a18aw661qz8is/pot_plastic.mov?dl=1 -o pot.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/1o60nbvionp4uu0/mimosa_nosound-400x.mp4?dl=1 -o mimosa.mov -s\n",
        "!curl -L https://www.dropbox.com/s/dun532o3s1c49xi/Codariocalyx%2BMimosa-400x.mp4?dl=1 -o mimosa_400x.mov -s\n",
        "!curl -L https://www.dropbox.com/s/lb24eyw57qdiocc/codariocalyx_nopeter_nosound-400x.mp4?dl=1 -o codariocalyx_no_peter.mov -s\n",
        "# !curl -L https://www.dropbox.com/s/1o60nbvionp4uu0/mimosa_nosound-400x.mp4?dl=1 -o mimosa_400x_no_sound.mov -s\n",
        "# # # Images\n",
        "# !curl -o happy.png -L https://i.imgur.com/PXpWO5Cs.png -s\n",
        "\n",
        "# !curl -o sad.png -L https://i.imgur.com/mAYh4Qts.png -s"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Videos...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2DVviYt51gI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title # Parameters, Constants\n",
        "\n",
        "Video = \"codariocalyx_no_peter.mov\"  #@param ['paprika.mov', 'pot.mov', 'mimosa.mov', 'mimosa_400x.mov', 'codariocalyx_no_peter.mov', 'mimosa_400x_no_sound.mov']\n",
        "INPUT_VIDEO = Video\n",
        "\n",
        "# Constants\n",
        "INPUT_VIDEO_NAME = 'pls_delete'\n",
        "OUTPUT_VIDEO_NAME_MPEG = 'pls_delete.mpeg'\n",
        "OUTPUT_VIDEO_NAME_MP4 = '{}.mp4'.format(INPUT_VIDEO_NAME)\n",
        "OUTPUT_VIDEO_NAME_WAV = '{}.wav'.format(INPUT_VIDEO_NAME)\n",
        "FRAMES_PER_SECOND = 30\n",
        "x = 0 \n",
        "y = 0 \n",
        "w = 0\n",
        "h = 0\n",
        "\n",
        "# Params\n",
        "Shrink_Regions_by_px =  12#@param {type:\"integer\"}\n",
        "ERODE_1 =  Shrink_Regions_by_px\n",
        "ERODE_2 = ERODE_1 \n",
        "\n",
        "# Dilate: Makes intersting regions grow.\n",
        "Grow_Regions_by_px =  17#@param {type:\"integer\"}\n",
        "DILATE_1 = Grow_Regions_by_px\n",
        "DILATE_2 = DILATE_1\n",
        "\n",
        "# Lumen_Threshold = 250#@param {type:\"integer\"} \n",
        "# MASK_THRESH = Lumen_Threshold\n",
        "Countour_Approximation = 60#@param {type:\"integer\"}\n",
        "CONTOUR_AREA = Countour_Approximation"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOtPzivUfYFl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title # Leaf Class hide\n",
        "\n",
        "class Leaf():\n",
        "    def __init__(self, id, hsv_frame, track_window):\n",
        "\n",
        "        self.id = id\n",
        "\n",
        "        self.track_window = track_window\n",
        "        self.term_crit = \\\n",
        "            (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 10, 1)\n",
        "\n",
        "        # Initialize the histogram.\n",
        "        self.x, self.y, self.w, self.h = track_window\n",
        "        roi = hsv_frame[y:y+h, x:x+w]\n",
        "        roi_hist = cv2.calcHist([roi], [0], None, [16], [0, 180])\n",
        "        self.roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255,\n",
        "                                      cv2.NORM_MINMAX)\n",
        "\n",
        "        # Initialize the Kalman filter.\n",
        "        self.kalman = cv2.KalmanFilter(4, 2)\n",
        "        self.kalman.measurementMatrix = np.array(\n",
        "            [[1, 0, 0, 0],\n",
        "             [0, 1, 0, 0]], np.float32)\n",
        "        self.kalman.transitionMatrix = np.array(\n",
        "            [[1, 0, 1, 0],\n",
        "             [0, 1, 0, 1],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 0, 1]], np.float32)\n",
        "        self.kalman.processNoiseCov = np.array(\n",
        "            [[1, 0, 0, 0],\n",
        "             [0, 1, 0, 0],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 0, 1]], np.float32) * 0.03\n",
        "        cx = x+w/2\n",
        "        cy = y+h/2\n",
        "        self.kalman.statePre = np.array(\n",
        "            [[cx], [cy], [0], [0]], np.float32)\n",
        "        self.kalman.statePost = np.array(\n",
        "            [[cx], [cy], [0], [0]], np.float32)\n",
        "        \n",
        "    def get_plant_values(self): \n",
        "        return np.array([ self.id, self.x, self.y, self.w, self.h ])\n",
        "    \n",
        "    def update(self, frame, hsv_frame):\n",
        "\n",
        "        back_proj = cv2.calcBackProject(\n",
        "            [hsv_frame], [0], self.roi_hist, [0, 180], 1)\n",
        "\n",
        "        ret, self.track_window = cv2.meanShift(\n",
        "            back_proj, self.track_window, self.term_crit)\n",
        "        x, y, w, h = self.track_window\n",
        "        center = np.array([x+w/2, y+h/2], np.float32)\n",
        "\n",
        "        prediction = self.kalman.predict()\n",
        "        estimate = self.kalman.correct(center)\n",
        "        center_offset = estimate[:,0][:2] - center\n",
        "        self.track_window = (x + int(center_offset[0]),\n",
        "                             y + int(center_offset[1]), w, h)\n",
        "        x, y, w, h = self.track_window\n",
        "\n",
        "        # Draw the predicted center position as a blue circle.\n",
        "        cv2.circle(frame, (int(prediction[0]), int(prediction[1])),\n",
        "                   4, (255, 0, 0), -1)\n",
        "\n",
        "        # Draw the corrected tracking window as a cyan rectangle.\n",
        "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "        # Draw the ID above the rectangle in blue text.\n",
        "        cv2.putText(frame, 'ID: %d' % self.id, (x, y-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0),\n",
        "                    1, cv2.LINE_AA)\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUY9869w7GZN",
        "colab_type": "text"
      },
      "source": [
        "# Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "610QWEp9LDpp",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "# @title ## Playground\n",
        "\n",
        "# # Object Detection different Thresholding Algorithms\n",
        "\n",
        "# # Load Video\n",
        "# cap = cv2.VideoCapture('mimosa_400x.mov')\n",
        "# arr = []\n",
        "# no_of_frame = 0\n",
        "# frame_width = int(cap.get(3))\n",
        "# frame_height = int(cap.get(4))\n",
        "\n",
        "# erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ERODE_1, ERODE_2))\n",
        "# dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (DILATE_1, DILATE_2))\n",
        "# num_history_frames_populated = 0\n",
        "\n",
        "# fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
        "# out = cv2.VideoWriter(OUTPUT_VIDEO_NAME_MPEG, fourcc, FRAMES_PER_SECOND, (frame_width, frame_height))\n",
        "# grabbed_frames = 0\n",
        "\n",
        "# # ---------Thresholding------------\n",
        "\n",
        "# grabbed, frame = cap.read()\n",
        "# frame = frame.copy()\n",
        "# frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# # Binary Thresholding\n",
        "# _, thresh = cv2.threshold(frame, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# # Adaptive Thresholding\n",
        "# thresh = cv2.adaptiveThreshold(frame, 120, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "# # Niblack and Sauvola Thresholding\n",
        "# binary_global = frame > threshold_otsu(frame)\n",
        "\n",
        "# window_size = 25\n",
        "# thresh_niblack = threshold_niblack(frame, window_size=window_size, k=0.8)\n",
        "# thresh_sauvola = threshold_sauvola(frame, window_size=window_size)\n",
        "\n",
        "# binary_niblack = frame > thresh_niblack\n",
        "# cv2_imshow(binary_niblack)\n",
        "# binary_sauvola = frame > thresh_sauvola\n",
        "# cv2_imshow(binary_sauvola)\n",
        "\n",
        "# # Find intersting points\n",
        "# gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "# corners = cv2.goodFeaturesToTrack(gray,25,0.01,10)\n",
        "# corners = np.int0(corners)\n",
        "\n",
        "# for i in corners:\n",
        "#     x,y = i.ravel()\n",
        "#     cv2.circle(gray,(x,y),3,255,-1)\n",
        "# # cv2_imshow(gray)\n",
        "\n",
        "# # Morphology\n",
        "# cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "# cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "# # cv2.morphologyEx(thresh, cv2.MORPH_OPEN, erode_kernel) \n",
        "# # cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, erode_kernel)\n",
        "\n",
        "# # Detect Contours in the thresholded image.\n",
        "\n",
        "# contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# # contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\n",
        "\n",
        "# hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    \n",
        "# cap.release()\n",
        "# out.release()\n",
        "# cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jUFAd_f6Kg-",
        "colab_type": "text"
      },
      "source": [
        "- [Link to Plantions Post](https://plantions.github.io/project/2020/06/30/sprint-5.html)\n",
        "\n",
        "- [Shi Tomasi](https://docs.opencv.org/master/d4/d8c/tutorial_py_shi_tomasi.html)\n",
        "- [Changing Video Resolution](https://theailearner.com/2018/11/15/changing-video-resolution-using-opencv-python/)\n",
        "- [Niblack and Sauvola Threshholding](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_niblack_sauvola.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4EkbtzAglB0",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "8031e176-02db-45b2-cdc6-1047a1fb3aae"
      },
      "source": [
        "# @title # Orig Code Hide\n",
        "\n",
        "# Clean Up Temporary Movie Files\n",
        "try:\n",
        "    os.remove(\"pls_delete.mp4\")\n",
        "    os.remove(\"pls_delete.mpeg\")\n",
        "    os.remove(\"pls_delete.wav\")\n",
        "    os.remove(\"pls_delete_play.mp4\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Downloading Videos...\")\n",
        "else:\n",
        "    print(\"RM. Downloading Videos...\")\n",
        "\n",
        "\n",
        "#@title # Step 2: Video Processing\n",
        "\n",
        "# Video Size Reducer\n",
        "print(\"Starting time of video reduction: {}\".format(datetime.now()))\n",
        "start_time = time.time()\n",
        "\n",
        "# Reduce video size to 480p\n",
        "cap = cv2.VideoCapture('mimosa_400x.mov')\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('reduced.avi',fourcc, 5, (640,480))\n",
        " \n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if ret == True:\n",
        "        b = cv2.resize(frame,(640,480),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "        out.write(b)\n",
        "    else:\n",
        "        break\n",
        "    \n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# print(\"Video quality reduction to 480p took\", round(((time.time() - start_time2)/60), 2), \"minutes.\")\n",
        "\n",
        "# Create a VideoCapture object from Reduced Video Resolution 480p\n",
        "cap = cv2.VideoCapture('reduced.avi')\n",
        "arr = []\n",
        "no_of_frame = 0\n",
        "no_of_frames_to_process = (int(cap.get(cv2.CAP_PROP_FRAME_COUNT))-4)\n",
        "duration_of_video_s = (int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))/30\n",
        "print('this is the duration: ', duration_of_video_s)\n",
        "\n",
        "# Capture several frames to allow the camera's autoexposure to # adjust.\n",
        "for i in range(3):\n",
        "    success, frame = cap.read() \n",
        "if not success:\n",
        "    exit(1)\n",
        "\n",
        "# Default resolutions of the frame are obtained and casted to int.\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "# Create the KNN background subtractor.\n",
        "bg_subtractor = cv2.createBackgroundSubtractorKNN(detectShadows=True)\n",
        "history_length = 5\n",
        "bg_subtractor.setHistory(history_length)\n",
        "\n",
        "erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 3))\n",
        "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "leafs = []\n",
        "num_history_frames_populated = 0\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_NAME_MPEG, fourcc, FRAMES_PER_SECOND, (frame_width, frame_height))\n",
        "\n",
        "grabbed_frames = 0\n",
        "\n",
        "# ---------DETECTION------------\n",
        "\n",
        "while True:\n",
        "    grabbed, frame = cap.read()\n",
        "    grabbed_frames += 1\n",
        "    no_of_frame = no_of_frame + 1\n",
        "    if no_of_frame == 400:\n",
        "        grabbed = False\n",
        "\n",
        "    if (grabbed is False):\n",
        "        break\n",
        "\n",
        "    # Apply the KNN background subtractor.\n",
        "    fg_mask = bg_subtractor.apply(frame)\n",
        "\n",
        "    # Let the background subtractor build up a history.\n",
        "    if num_history_frames_populated < history_length:\n",
        "        num_history_frames_populated += 1\n",
        "        continue\n",
        "\n",
        "    # Create the thresholded image.\n",
        "    \n",
        "    # Thresholding\n",
        "    # _, thresh = cv2.threshold(fg_mask, 127, 255, cv2.THRESH_BINARY)\n",
        "    thresh = cv2.adaptiveThreshold(fg_mask, 120, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    \n",
        "    # Find intersting points\n",
        "    # thresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.goodFeaturesToTrack(thresh,25,0.01,100)\n",
        "    \n",
        "\n",
        "    # for i in corners:\n",
        "    #     x,y = i.ravel()\n",
        "    #     cv2.circle(gray,(x,y),3,255,-1)\n",
        "    # cv2_imshow(frame)\n",
        "\n",
        "    # Morphology\n",
        "    # cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "    # cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "    cv2.morphologyEx(thresh, cv2.MORPH_OPEN, erode_kernel) \n",
        "    # cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, erode_kernel)\n",
        "    \n",
        "    # Detect Contours in the thresholded image.\n",
        "    \n",
        "    contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Draw green rectangles around large contours.\n",
        "    # Also, if no leafs are being tracked yet, create some.\n",
        "    should_initialize_leafs = len(leafs) == 0\n",
        "    id = 0\n",
        "    print (\"\\r Processing {} of {} frames.\".format(no_of_frame, no_of_frames_to_process), end=\"\")\n",
        "    # print('Number of frames to complete: ',no_of_frames_to_process)\n",
        "    \n",
        "# ---------TRACKING---------    \n",
        "\n",
        "    for c in contours:\n",
        "        if cv2.contourArea(c) > 100:\n",
        "            (x, y, w, h) = cv2.boundingRect(c)\n",
        "            cv2.circle(frame, (x, y), 3,\n",
        "                            (0, 255, 0), 1)\n",
        "            if should_initialize_leafs:\n",
        "                leafs.append(Leaf(id, hsv_frame, (x, y, w, h)))\n",
        "        frame_no_and_values = np.array([ id, x, y, w, h ])\n",
        "        a = np.append(frame_no_and_values, [no_of_frame])\n",
        "        arr.append(a)\n",
        "        id += 1\n",
        "        \n",
        "    # Update the tracking of each leaf.\n",
        "    for leaf in leafs:\n",
        "        leaf.update(frame, hsv_frame)\n",
        "        frame_no_and_values = np.array([ id, x, y, w, h ])\n",
        "        a = np.append(frame_no_and_values, [no_of_frame])\n",
        "        arr.append(a)\n",
        "    out.write(frame)\n",
        "    # Print Image every 3 seconds (i.e. modulo 90 frames)\n",
        "    # if no_of_frame%4 == 0:\n",
        "        # cv2_imshow(frame)\n",
        "        # cv2_imshow(thresh)\n",
        "        # cv2_imshow(hsv_frame)\n",
        "        \n",
        "    \n",
        "cap.release()\n",
        "\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "motion_array = np.array(arr)\n",
        "\n",
        "print(\"\\nProcessing took\", round(((time.time() - start_time)/60), 2), \"minutes.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Videos...\n",
            "Starting time of video reduction: 2020-07-21 18:00:44.227797\n",
            "this is the duration:  1.5666666666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1b02e5af7c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Detect Contours in the thresholded image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_EXTERNAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;31m# contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mhsv_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2HSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/contours.cpp:197: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2n8IPu2K9Oy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5e245777-c7a5-47bf-ecd1-010ca629997f"
      },
      "source": [
        "### Movie Conversion from .mpeg to .mp4\n",
        "start_time2 = time.time()\n",
        "# Shortened videos to 15s to display them in Preview\n",
        "subprocess.run('ffmpeg -i {} {} -loglevel quiet'.format(OUTPUT_VIDEO_NAME_MPEG, OUTPUT_VIDEO_NAME_MP4), shell=True)\n",
        "\n",
        "# Shortening Duration of Video for Preview to max 15s or \n",
        "time_subvideo_to_be_converted = min(duration_of_video_s, 15)\n",
        "round(time_subvideo_to_be_converted, 1) \n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "ffmpeg_extract_subclip('pls_delete.mp4', 0, time_subvideo_to_be_converted, targetname=\"pls_delete_play.mp4\",)\n",
        "print(\"Conversion to mp4 and creating preview took\", round(((time.time() - start_time2)/60), 2), \"minutes.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[MoviePy] Running:\n",
            ">>> /usr/bin/ffmpeg -y -i pls_delete.mp4 -ss 0.00 -t 1.57 -vcodec copy -acodec copy pls_delete_play.mp4\n",
            "... command successful.\n",
            "Conversion to mp4 and creating preview took 0.03 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_DT6YZOl6eV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "78520481-de32-4d8e-f7d9-ac82710189e4"
      },
      "source": [
        "# @title #Step 3: Preview the Video\n",
        "\n",
        "def playvideo(filename):\n",
        "    video = io.open(filename, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    return HTML(data='''<video width=\"80%\" style=\"display:block; margin:0 auto;\" alt=\"test\" controls>\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "                 </video>'''.format(encoded.decode('ascii')))\n",
        "playvideo('pls_delete_play.mp4')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video width=\"80%\" style=\"display:block; margin:0 auto;\" alt=\"test\" controls>\n",
              "                    <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAAhtZGF0AAAA1m1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAAAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU3LjgzLjEwMA==\" type=\"video/mp4\"/>\n",
              "                 </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL7ZcQFkBpiM",
        "colab_type": "text"
      },
      "source": [
        "## Detection of Object 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUAJSMk_B8hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "bg_subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
        "erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eNvtgOlCQ07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap = cv2.VideoCapture('paprika.mov')\n",
        "for i in range(10):\n",
        "    success, frame = cap.read()\n",
        "if not success:\n",
        "    exit(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKtB8CwBCjdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gray_background = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
        "# gray_background = cv2.GaussianBlur(gray_background, (BLUR_RADIUS, BLUR_RADIUS), 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCuC8-bYCtQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# success, frame = cap.read() \n",
        "# while success:\n",
        "#     # gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
        "#     # gray_frame = cv2.GaussianBlur(gray_frame, (BLUR_RADIUS, BLUR_RADIUS), 0)\n",
        "#     # diff = cv2.absdiff(gray_background, gray_frame)\n",
        "#     fg_mask = bg_subtractor.apply(frame)\n",
        "#     _, thresh = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)\n",
        "#     cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "#     cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "#     contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "#     for c in contours:\n",
        "#         if cv2.contourArea(c) > 500:\n",
        "#             x, y, w, h = cv2.boundingRect(c)\n",
        "#             cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "#     # cv2_imshow(fg_mask) \n",
        "#     # cv2_imshow(thresh) \n",
        "#     # cv2_imshow(frame)\n",
        "\n",
        "# k = cv2.waitKey(1) \n",
        "# if k == 27: # Escape\n",
        "#     break\n",
        "\n",
        "# success, frame = cap.read()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf6Gggtm4zI5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title # Step 4: Data Visualizations\n",
        "\n",
        "# ____ VIDEO PROCESSING ____\n",
        "# This is the output NumPy Array from the video processing\n",
        "data = motion_array\n",
        "\n",
        "# Create a Pandas DataFrame from NumPy arrays with UniqueID, x- & y-Value of ROI (i.e., Region of Interest), as well as width and height of the accompanying moving windows. \n",
        "# Frame refers to video frame (30 fps) on which point was tracked.\n",
        "# RESET DF\n",
        "df_video = pd.DataFrame(data=data[0:,0:], index=[i for i in range(data.shape[0])], columns=['ID','X','Y','W','H','Frame'])\n",
        "\n",
        "# Calculate Elapsed Time for Frames of Video\n",
        "df_video[\"Elapsed\"] = round(df_video[\"Frame\"]/30, 1)\n",
        "\n",
        "# Remove Duplicates\n",
        "df_video_wo_dups = df_video.drop_duplicates(subset=None, keep='first', inplace=False)\n",
        "df_video_queried = df_video_wo_dups.query('0 <= ID <= {}'.format(10))\n",
        "# df_video_4_cols = df_video_queried.filter(items=['ID', 'X', 'Y', 'Elapsed'])\n",
        "df_video_3_cols = df_video_queried.filter(items=['ID', 'Y', 'Elapsed'])\n",
        "\n",
        "# Minding that Matplotlib inverts x/y Values\n",
        "# df_inverted_x_y_minus_1 = df_video_4_cols.sub([0, frame_width, frame_height, 0], axis='columns')\n",
        "df_inverted_x_y_minus_1 = df_video_3_cols.sub([0, frame_height, 0], axis='columns')\n",
        "# df_inverted_x_y = df_inverted_x_y_minus_1.mul([1, -1, -1, 1], axis='columns')\n",
        "df_inverted_x_y = df_inverted_x_y_minus_1.mul([1, -1, 1], axis='columns')\n",
        "df_video = df_inverted_x_y\n",
        "\n",
        "# Use Pivot Feature of Pandas DataFrame\n",
        "# df_pivoted_video = pd.pivot_table(df_video, values=[\"X\", \"Y\"], index=pd.Grouper(key='Elapsed'), columns=[\"ID\"],)\n",
        "df_pivoted_video = pd.pivot_table(df_video, values=[\"Y\"], index=pd.Grouper(key='Elapsed'), columns=[\"ID\"],)\n",
        "\n",
        "# Create flexible number of indices based on number of Regions of Interest\n",
        "int(df_video_queried.max(axis = 0)[0]); counter = 0\n",
        "# x_indices = []\n",
        "y_indices = []\n",
        "while counter <= int(df_video_queried.max(axis = 0)[0]):\n",
        "    # x_indices.append(('X' + str(counter))); \n",
        "    y_indices.append(('Y' + str(counter)))\n",
        "    counter = counter + 1\n",
        "df_pivoted_video.columns = y_indices # + x_indices\n",
        "\n",
        "# ____AUDIO PROCESSING____\n",
        "\n",
        "# Extracts Audio from Input Video\n",
        "command = \"ffmpeg -i {} -ab 160k -ac 2 -ar 44100 -vn {}\".format(INPUT_VIDEO, OUTPUT_VIDEO_NAME_WAV)\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "# Unpack Audio\n",
        "audio_path = INPUT_VIDEO\n",
        "x , sr = librosa.load(audio_path)\n",
        "\n",
        "# Get the MFCCs (and choose the number of features to extract); manipulate Pandas DataFrame.\n",
        "mfccs = librosa.feature.mfcc(x, sr=sr, n_mfcc=4)\n",
        "audio_data = np.transpose(mfccs)\n",
        "\n",
        "# Calculate the step length between a mfcc and the audio in seconds\n",
        "window_length = 512 # default value by mfcc algorithm\n",
        "length = (round((window_length/sr)*mfccs.shape[1]), 2)\n",
        "step_size = length[0]/mfccs.shape[1]\n",
        "\n",
        "# Getting Elapsed Time of Audio from Audiofile\n",
        "audio_time_data = np.arange(0, (length[0]), step_size)\n",
        "audio_time_data = np.round(audio_time_data, 1)\n",
        "df_audio_time_steps = pd.DataFrame(data=audio_time_data, \n",
        "                index=[i for i in range(audio_time_data.shape[0])], \n",
        "                columns=['Elapsed_t'])\n",
        "# Create the Audio Pandas DataFrame\n",
        "df_audio = pd.DataFrame(data=audio_data, \n",
        "                index=[i for i in range(audio_data.shape[0])], \n",
        "                columns=['MFCC'+str(i) for i in range(audio_data.shape[1])])\n",
        "df_audio_w_time = pd.concat([df_audio_time_steps, df_audio], axis=1, join='outer')\n",
        "df_audio_w_time_w_o_dups = df_audio_w_time.drop_duplicates(subset='Elapsed_t', keep='first')\n",
        "df_audio_w_time_w_o_dups\n",
        "\n",
        "## Merge AUDIO and VIDEO\n",
        "# df_merged = pd.concat([df_audio_w_time_w_o_dups, df_pivoted_video], axis = 1, join= 'outer')\n",
        "df_merged = pd.merge(df_audio_w_time_w_o_dups, df_pivoted_video, how='left', left_on=['Elapsed_t'], right_on=['Elapsed']) # add 'indicator=True' to see functionality\n",
        "df_bf_filled = df_merged.fillna(method='bfill') # backward filling (NANs filling up)\n",
        "df_filled = df_bf_filled.fillna(method='ffill') # forward filling (NANs filling down)\n",
        "\n",
        "# Melt from wide to long-form Data Structure\n",
        "df_filled_every_two_seconds = df_filled.iloc[::20]\n",
        "df_filled_melted = df_filled_every_two_seconds.melt('Elapsed_t', var_name='Variables', value_name='Values')\n",
        "\n",
        "alt.Chart(df_filled_melted).mark_line().encode(\n",
        "  x='Elapsed_t',\n",
        "  y='Values',\n",
        "  color='Variables'\n",
        "  ).properties(\n",
        "    width=900,\n",
        "    height=400,\n",
        ").interactive(bind_y=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJXLWK_WA21",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title # Step 5: Happiness Indicator\n",
        "\n",
        "total_happiness_counter = 0\n",
        "per_curve_happiness_counter = 0\n",
        "img_happy = cv2.imread('happy.png', cv2.IMREAD_UNCHANGED)\n",
        "img_sad = cv2.imread('sad.png', cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "# obviously one cannot assume that the height is fixed\n",
        "HEIGHT = 100\n",
        "# HEIGHT = (max(x[peaks])/2 )\n",
        "\n",
        "# To Do: has to be a regex and start with y(i) values\n",
        "for tracked_move in df_filled.columns[5:]:\n",
        "    x = df_filled[tracked_move]\n",
        "    # try without height bc its an absolute value\n",
        "    peaks, _ = find_peaks(x, HEIGHT, distance=50)\n",
        "    per_curve_happiness_counter = 0\n",
        "    per_curve_happiness_indicators = len(x[peaks])\n",
        "    happiness_threshold = (max(x[peaks]) + min(x[peaks]))/2\n",
        "    \n",
        "    for value in x[peaks]:\n",
        "        if value > happiness_threshold:\n",
        "            per_curve_happiness_counter = per_curve_happiness_counter + 1\n",
        "            total_happiness_counter = total_happiness_counter + 1\n",
        "\n",
        "if per_curve_happiness_counter > (per_curve_happiness_indicators/2):\n",
        "    print(\"This plant observed that the speaker is >>> happy <<<.\")\n",
        "    # cv2_imshow(img_happy)\n",
        "else:\n",
        "    print(\"This plant observed that the speaker is >>> sad <<<.\")\n",
        "    # cv2_imshow(img_sad)\n",
        "\n",
        "plt.plot(x)\n",
        "plt.plot(peaks, x[peaks], \"x\")\n",
        "plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7teKtpm0gzxh",
        "colab_type": "text"
      },
      "source": [
        "# Additional Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6CAmpCHTZ3G",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Dummy Sad Plant\n",
        "\n",
        "img_happy = cv2.imread('happy.png', cv2.IMREAD_UNCHANGED)\n",
        "img_sad = cv2.imread('sad.png', cv2.IMREAD_UNCHANGED)\n",
        "print(\"This plant observed that the speaker is >>> sad <<<.\")\n",
        "cv2_imshow(img_sad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZYb3cMZPMLB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Camera Capturing [in Process]\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "\n",
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_q_b5d6g2L7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Export to Excel\n",
        "df_filled.to_excel('11_min_video.xlsx')\n",
        "df_filled_melted.to_excel('11_min_video_melted.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-3S5srS3uVc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Interactive Scatter Plot\n",
        "\n",
        "# df_filled\n",
        "\n",
        "# alt.Chart(df_filled_melted).mark_circle().encode(\n",
        "#     alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
        "#     alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
        "# ).properties(\n",
        "#     width=150,\n",
        "#     height=150\n",
        "# ).repeat(\n",
        "#     row=['Elapsed_t', 'Variables', 'Values'],\n",
        "#     column=['Variables', 'Elapsed_t', 'Values']\n",
        "# ).interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFQ8iyIwkYhy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Check for Movie Duration\n",
        "import moviepy.editor\n",
        "\n",
        "# Converts into more readable format\n",
        "def convert(seconds):\n",
        "    hours = seconds // 3600\n",
        "    seconds %= 3600\n",
        "\n",
        "    mins = seconds // 60\n",
        "    seconds %= 60\n",
        "\n",
        "    return hours, mins, seconds\n",
        "\n",
        "# Duration of Input Video\n",
        "video = moviepy.editor.VideoFileClip(INPUT_VIDEO)\n",
        "video_duration = float(video.duration)\n",
        "hours, mins, secs = convert(video_duration)\n",
        "print(\"The Video named {} Seconds:\".format(INPUT_VIDEO), secs)\n",
        "\n",
        "# Duration of Resulting Video\n",
        "video2 = moviepy.editor.VideoFileClip(OUTPUT_VIDEO_NAME_MP4)\n",
        "video_duration2 = float(video2.duration)\n",
        "hours, mins, secs = convert(video_duration2)\n",
        "print(\"The Video named {} Seconds:\".format(OUTPUT_VIDEO_NAME_MP4), secs)\n",
        "\n",
        "# Cross-check Length of Video and Audio\n",
        "print(\"Length of Audio in Seconds: \", (512/sr)*mfccs.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkQfbg_hTvT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Getting FPS of Video\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "\n",
        "    # Start default camera\n",
        "    video = cv2.VideoCapture(INPUT_VIDEO)\n",
        "\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "\n",
        "    # Release video\n",
        "    video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk2QspWq5egg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## File Uploader\n",
        "# Imports\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "import librosa\n",
        "import librosa.display\n",
        "import altair as alt\n",
        "from scipy.signal import chirp, find_peaks, peak_widths\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "!curl -o happy.png -L https://i.imgur.com/PXpWO5Cs.png -s\n",
        "!curl -o sad.png -L https://i.imgur.com/mAYh4Qts.png -s\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('Successfully uploaded file \"{NAME_OF_UPLOADED_VIDEO}\".'.format(\n",
        "      NAME_OF_UPLOADED_VIDEO=fn,))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQQmAPzz582U",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title ## Audio File Player\n",
        "\n",
        "# Needed imports\n",
        "from IPython.display import Audio\n",
        "\n",
        "audio_name = 'test2.mp3'\n",
        "\n",
        "# Generate a player for mono sound\n",
        "Audio(audio_name,autoplay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_IRN0d9iZzt",
        "colab_type": "text"
      },
      "source": [
        "Please send Feedback to sduerr@mit.edu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3S09aDECrzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}